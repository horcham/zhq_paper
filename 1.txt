分类号: 密级:
UDC 学号: 20142210069
South China Normal University
学 士 学 位 论 文
基于神经网络的花苗分类
学位申请人
张泓铨
专业学位名称 信息与计算科学
专业学位领域 理学
所在院系
数学科学学院
导师姓名及职称
杨坦 讲师
2018 年 4 月 16 日
公开基于神经网络的花苗分类
基于神经网络的花苗分类
专业名称: 信息与计算科学
摘
申请者: 张泓铨
导师: 杨坦 讲师
要
华南师范大学是一所有着悠久历史和深厚人文底蕴的高等学府,学校地处中
国改革开放之都——广州,深得岭南开放务实之精神,有着素朴的传统和良好的
学风,七十多年来,学校坚持师范教育的特色,始终致力于培养人格健全、有思
想、有能力、有社会责任感的优秀人才。
在大学日益参与经济社会发展的新世纪,华南师范大学正以崭新的风貌、开
阔的世界眼光,不断拓展大学的育人理念,创造良好的教学和包容的学术研究环
境,营造丰富的校园文化,努力构建特色鲜明、开放式、综合性高水平教学研究
型大学。
关键词: 华南师范大学;论文;模板
第 I 页基于神经网络的花苗分类
Plant Seedling Classification
Based on Neural Network
Major: Information and Computing Scinece
Name: Hongquan Zhang
Supervisor: Tan Yang
ABSTRACT
South China Normal University is an institution of higher education with a long his-
tory and a rich legacy. Situated in Guangzhou, the open metropolis in South China, South
China Normal University is imbued with Lingnan’s pioneering and pragmatic spirits. It
has developed a tradition of elegant simplicity and fostered a strong learning environ-
ment. In the past seventy years or so, South China Normal University has preserved its
characteristics of teacher education and has been devoted to cultivating talents with moral
integrity, independent thinking, innovative ability and sense of social responsibility.
In the new century in which the institutions of higher education are getting increas-
ingly involved in the social and economic development, South China Normal University
has adopted an international view on education. Having broadened its horizon on the
key issue of cultivating talents, it has made its greatest efforts to create a congenial and
harmonious environment for both teaching and academic research and has fostered a rich
variety of campus culture. It aims to build itself into a high-level comprehensive teaching
and research-oriented university with open distinctive features.
KEY WORDS: SCNU; thesis; template
第 III 页基于神经网络的花苗分类
目 录
摘
要 ........................................................................................................................
I
ABSTRACT ................................................................................................................ III
第 1 章 绪论 ............................................................................................................ 1
第 2 章 论文正文 .................................................................................................... 3
2.1 花苗分类问题 ........................................................................................... 3
2.2 数据预处理 ............................................................................................... 5
2.2.1 掩模构建与形态学去噪 ................................................................ 5
2.2.2 形态学去噪 .................................................................................... 6
2.2.3 边框裁剪与尺寸统一化 ................................................................ 7
BP 神经网络 .............................................................................................. 8
2.3.1 神经网络的构造与前向传播 ........................................................ 8
2.3.2 神经网络的反向传播 .................................................................... 10
2.3.3 激活函数 ........................................................................................ 14
2.3.4 传统 BP 网络的应用 ...................................................................... 16
2.3.5 梯度下降方法 ................................................................................ 18
2.3.6 正则化与 dropout ........................................................................... 20
2.3.7 BP 神经网络 +X ............................................................................ 22
2.3
2.4
卷积神经网络 ........................................................................................... 25
2.4.1 卷积神经网络概述 ........................................................................ 25
2.4.2 经典 CNN 模型 .............................................................................. 28
2.4.3 CNN 的应用 ................................................................................... 29
2.4.4 CNN+X ........................................................................................... 31
2.5 参考文献 ................................................................................................... 32
第 3 章 结论 ............................................................................................................ 33
附录 A 外文资料原文 ............................................................................................. 35
A.1
First Principles .......................................................................................... 35
A.1.1 Typography exists to honor content. .............................................. 35
A.1.2 Letters have a life and dignity of their own. .................................. 36
第 V 页基于神经网络的花苗分类
附录 B
致
其它附录 ..................................................................................................... 39
谢 ........................................................................................................................ 41
作者攻读学位期间发表的学术论文目录 ................................................................ 43
第 VI 页基于神经网络的花苗分类
第 1 章
绪论
大量数据代表了价值。数据背后通常隐含着客观规律,如果数据量足够大的
话,其规律是可以被认知和学习的,其催生了机器学习的研究方向,研究如何用
数据进行建模与变现。然而,由于数据量极大,而且所涉及的算法会很复杂,通
常不可能进行人为的计算,即使是用计算机进行计算,也对计算机的处理速度,
内存,储存空间提出了一定的要求。另一方面,如若要进行机器学习,除了计算
机硬件的要求之外,还需要软件与算法的支持,其中,算法是机器学习的核心。
历史发展来看,计算机硬件,用于机器学习的软件与算法的发展是相辅相成的。
在 20 世纪 40 年代,人们开始研究人工智能,由于生物学的发展,人们模
仿人类的神经元运作而提出了神经网络的原型:M-P 神经元模型,并提出了激活
函数的概念。在 20 世纪 50 年代到 60 年代,感知器算法、梯度下降法、最小二
乘法等求解算法面世,而且提出了感知器,并开始应用在文字、语音、信号等领
域。在 20 世纪 60 年代到 70 年代,神经网络算法因感知器的缺陷而衰落。在 70
年代到 80 年代,神经网络的种类变得丰富起来,涌现出 BP 神经网络,RBF 神经
网络等各种网络,并提出了深度学习的概念与卷积神经网络(CNN)和循环神经
网络(RNN)的结构。90 年代后,一些有别于神经网络的算法面世,如 SVM,决
策树,boosting 与随机森林等方法,从不同的角度对机器学习算法进行丰富。在
2006 年,Hinton 提出了解决深度学习中梯度消失问题的解决方法之后,深度学习
开始爆发。2012 年,ReLU 激活函数的提出,进一步抑制了梯度消失的问题,并
且深度学习在语音和图像方面开始有惊人的表现。2012 年,在 ImageNet 图像识别
比赛上,AlexNet 通过构建一定深度的 CNN 夺得冠军,其性能彻底击败了 SVM。
需注意的是,AlexNet 首次使用了 ReLU 激活函数,Dropout 防止过拟合方法,以
及 GPU 加速。之后,在 AlexNet 的结构上做优化,又提出了其他更强大的模型,
如 VGGNet,Inception 系列,ResNet 等。强化学习和迁移学习的提出,进一步增
强了模型的性能。
本论文基于 kaggle(全球数据科学平台)的花苗分类竞赛 (Plant Seedlings
第 1 页基于神经网络的花苗分类
Classification 1 ) 中的数据集,探究传统机器学习算法(SVM,决策树,随机森林与
boosting 等)、深度学习算法(AlexNet,VGGNet,InceptionV3)的原理与性能,
并对其尝试做优化与结合(如 AlexNet+SVM 等)。
1
https://www.kaggle.com/c/plant-seedlings-classification
第 2 页基于神经网络的花苗分类
第 2 章
2.1
论文正文
花苗分类问题
该问题来自于 kaggle 的 Plant Seedlings Classification 竞赛。给定的 13 类花苗
(有枝干,树叶,无花)的四千余张彩色图片用于训练、构建模型。其数据基本情
况如下
序号 种类 样本数量
1 Black-grass 263
2 Charlock 390
3 Cleavers 287
4 Common Chickweed 611
5 Common wheat 221
6 Fat Hen 475
7 Loose Silky-bent 654
8 Maize 221
9 Scentless Mayweed 516
10 Shepherds Purse 231
11 Small-flowered Cranesbill 496
12 Sugar beet 385
总和 – 4750
而且每一张的图片大小都有可能不同。每一类的图像的例子如下
Black-grass,Charlock,Cleavers
第 3 页基于神经网络的花苗分类
Common Chickweed,Common wheat,Fat Hen
Loose Silky-bent,Maize,Scentless Mayweed
Shepherds Purse,Small-flowered Cranesbill,Sugar beet
由于所涉及到的数据为彩色图像,而花苗的特征为绿色,故考虑使用 opencv
的方法,通过建立掩膜筛选出花苗的图像,进而将彩色图像转化为灰度图像或二
值图像,从而达到降维的目的。而即使降维之后,为了确保图像失真不大,所以
至少将图像转化为 64 × 64 的灰度图像矩阵。若考虑直接采用 Logistic、SVM 或决
策树方法,则需要将 64 × 64 的灰度图像矩阵拉伸为 4096 × 1 的图像,然而假设用
全部的数据进行训练,也只有 4750 个样本,训练时极容易导致过拟合,但是,考
虑集成学习的方法或是带 dropout 的 BP 神经网络可以一定程度上防止过拟合。考
虑到为涉及图像的分类问题,可以采用卷积神经网络(CNN)。由于是 13 类的分
类问题,且样本数较少,可以进一步考虑在用 SVM、Logistic 或决策树方法来替
代神经网络最后的 softmax 层,或许能起到更好的效果。
第 4 页基于神经网络的花苗分类
2.2
2.2.1
数据预处理
掩模构建与形态学去噪
对于花苗图像,我们可以看到,花苗的背景通常为黄土、砂砾或塑料箱等,
而绿色的花苗则显得非常好辨认。而且我们面对的花苗是绿色的,因而考虑设置
一个 hsv 范围,将绿色的部分从图像中剥离出来。于是我们首先将花苗图像进行
颜色空间的转换,从 rgb 颜色空间转化为 hsv 颜色空间,之后设定 hsv 颜色空间为
[26,43,46] 和 [99,255,255],在原图中过滤出在这个 hsv 颜色空间的图像得到掩膜,
若在这个区间中,则为白色,否则为黑色。之后可以通过该掩膜对原图进行位运
算,则可得到原图的图像。其部分代码如下
1
2
3
4
5
6
7
8
9
10
11
import cv2
# 假设待处理的图像为img
# rgb图像转化为hsv图像
hsv = hsv = cv2. cvtColor (img,cv2.COLOR_BGR2HLS)
# 绿色的阈值(HSV)
lower_green = np. array ([26,43,46])
upper_green = np. array ([99,255,255])
# 根据阈值构建掩膜
mask = cv2.inRange(hsv,lower_green,upper_green)
# 对原图像和掩膜进行位运算
res = cv2. bitwise_and (img,img,mask=mask)
得到的结果如下,以 Black-grass,Charlock,Cleavers 种类的各一个图像为例
Black-grass 的一个图像,其大小为 370 × 368,从左到有依次为原图,掩膜图像,
通过掩膜对原图过滤的图像
第 5 页基于神经网络的花苗分类
Charlock 的一个图像,其大小为 484 × 484,从左到有依次为原图,掩膜图像,通
过掩膜对原图过滤的图像
Cleavers 的一个图像,其大小为 346 × 346,从左到有依次为原图,掩膜图像,通
过掩膜对原图过滤的图像
2.2.2
形态学去噪
对于用掩模处理后的花苗二值图像,考虑到在花苗所在盆栽可能会有一些小
草,通过掩模处理后会有噪声。因而考虑用形态学方法去噪。
对于一个二值图像,比较常用的去噪方法是形态学去噪,而这通常涉及两种
形态学转换,分别为腐蚀和膨胀,其涉及的原理较简单。对于腐蚀,先定义一个
窗口,窗口将沿着图像滑动,以遍历整个图像。滑动过程中,窗口内所有像素不
全为 1 时,则令窗口中的所有像素等于 0;若窗口内所有像素全为 1, 时,则不做
操作。选用一个合适尺寸的窗口,对于腐蚀之后的图片,其白噪声点可以消除,
但也会对物体的边缘进行腐蚀。膨胀则与腐蚀相反,区别在于滑动过程中,窗口
内元素只要有 1,则整个窗口元素都令为 1,这样会增大物体的尺寸。通常对于有
白噪声的图片,先腐蚀再膨胀可以消除白噪声,但一定程度会导致物体失真。但
由于用掩模处理后的图像,其物体十分明显,用形态学方法去噪后失真的可能性
不大。因而考虑用形态学方法去噪。
第 6 页基于神经网络的花苗分类
2.2.3
边框裁剪与尺寸统一化
我们从掩膜图像可以看出,目标图像(白色部分)只是占所有图像的很小一
部分,而其余其余部分为黑色,而这其余的部分往往是无效的。现在考虑用一个
矩形边框去提取出图像的有效区域,而将无效区域剔除。方法是访问图像中有效
区域的宽度最小值和最大值,以及高度最小值和最大值,从而确定矩形边框区域。
其效果如下图所示
Sugar beet 的一个图像,其大小为 546 × 546,从左到有依次为原图,掩膜图像,对
掩膜图像矩形裁剪之后的图像,其大小为 199 × 530
实现这个效果的代码如下
1
2
3
4
5
import cv2
# 获得矩形边框的最小宽高以及宽度和高度
x,y,w,h = cv2.boundingRect(mask_temp)
# 在原掩膜图像中选取矩形边框
mask_tg = mask[y:(y+h),x :( x+w)]
由于卷积神经网络需要同样大小尺寸的输入,所以考虑将图像统一尺寸归一化为
统一的大小。内插的方法是 INTER-CUBIC,其结果如下图
Sugar beet 的一个图像,左边为矩形裁剪之后的图像,大小为 199 × 530,右边为
尺寸统一化后的图像,大小为 96 × 96 实现这个效果的代码如下
第 7 页基于神经网络的花苗分类
1 import cv2
2 # 尺寸变换
3 mask_tg_rs = cv2. resize (mask_tg ,[96,96], interpolation =cv2.INTER_CUBIC)
最后,对所有的图片都做上述操作。每张图片的掩膜的尺寸归一后的图像作为输
入,需要注意的是,图片格式为 uint8,需要转化为 float 才不会出问题,用一下代
码可以解决该问题。
1 from skimage import img_as_float
2 mask_tg_rs = img_as_float (mask_tg_rs)
输出则采用将名字用 one-hot 编码作为标签,采用自助法分割训练集和验证集。
2.3
2.3.1
BP 神经网络
神经网络的构造与前向传播
神经网络是由单个或多个神经元组成。下面是单个神经元的构造。
图 1:神经元
该神经元的输入由三个数据 x 1 , x 2 , x 3 以及偏置项 (bias)+1 组成,通过神经元后输
出的表达式为
h W,b (x) = f (W T x + b) = f (
3
∑
W i x i + b)
(2.1)
i=1
其中 f 为激活函数。激活函数是为了将线性项 W T x 变换为非线性。在 BP 中,较
常用的激活函数为 sigmoid 函数,其表达式如下
f (z) =
1
1 + exp(−z)
第 8 页
(2.2)基于神经网络的花苗分类
另外,令 b = w 0 ,则可重新定义 W = (w 0 , w 1 , w 2 , w 3 ) T ,x = (1, x 1 , x 2 , x 3 ),于是可将
上式写为
h W,b (x) = f (W T x)
(2.3)
下面讨论神经网络。多个神经元可以组成一个层,多个层互相连接可以组成神经
网络。其中,接受数据输入的层为输入层,数据计算后的数据的输出层,中间的
层则称为隐含层。为下图是含有两个隐含层的神经网络。
图 2:含有两个隐含层的神经网络
如图,最左边的为输入层,即图中的 Layer L1,最右边的为输出层,即图中的
Layer L4,中间的所有层,即图中的 Layer L2,Layer L3 为隐含层。
我们用 n l 来表示网络的层数,记第 i 层为 L i ,于是输入层为 L 1 ,输出层为
L n l 。由于神经网络可以有任意多的隐层以及隐藏神经元,则我们记 W i (l) j 为第 l 层
第 j 单元以及第 l + 1 层第 i 单元之间的连接权重,b (l)
i 为第 L + 1 层第 i 单元的偏
执。我们用 a (l)
i 表示第 l 层第 i 单元的激活值(输出值),则有
a (l+1)
i
= f (
S l
∑
(l)
W i (l) j a (l)
j + b i )
(2.4)
j=1
其中当 l = 1 时,a (l) = x,x 为输入向量 (x 1 , x 2 , · · · , x S l ),S l 指第 l 层的神经元个数,
我们用 z (l+1)
表示第 l + 1 层第 i 单元输入加权和(包括偏置),即
i
z (l+1)
i
=
S t
∑
(l)
W i (l) j a (l)
j + b i
j=1
第 9 页
(2.5)基于神经网络的花苗分类
则有
a (l+1)
= f (z (l+1)
)
i
i
h W,b (x) = a (n l ) = f (z (n l ) )
(2.6)
(2.7)
上述过程称为神经网络的前向传播。
2.3.2
神经网络的反向传播
根据上面的前向传播,我们设神经网络的各层表示为 L 1 , L 2 , · · · , L n l ,其中,
L n l 为输出层,对于输出层,假设输出层输出为 t = a (n l ) ,y 为标签,则若为回归问
题,则代价函数使用 MSE,即
1
J(W, b; x, y) = ||t − y|| 2
2
(2.8)
接下来计算输出层的残差
∂
δ i (n l ) =
J(W, b; x, y)
l )
∂z (n
i
∂ 1
= (n ) ||y − h W,b (x)|| 2
∂z l 2
i
∂ 1 ∑
(n l ) 2
= (n )
n l (y i − a j )
l 2
∂z
j=1
S
i
(2.9)
∂ 1 ∑
(n l ) 2
n l (y i − f (z i ))
(n l ) 2
∂z
j=1
S
=
i
′ (n l )
l )
= −(y i − f (z (n
i )) · f (z i )
′ (n l )
l )
= −(y i − a (n
i ) · f (z i )
下面考虑残差的递推算法,以输出层前一层为例。由前向传播我们可以推导出
z (l+1)
i
=
S l
∑
(l)
W i (l) j f (z (l)
i ) + b i
j=1
第 10 页
(2.10)基于神经网络的花苗分类
则有
z i (n i ) =
S l
∑
l −1)
l −1)
W i (n j l −1) f (z (n
) + b (n
i
i
(2.11)
j=1
于是有
l )
∂z (n
i
=
∂z i (n l −1)
S l
∑
l −1)
W i (n j l −1) f ′ (z (n
)
i
(2.12)
j=1
则可以得到输出层前一层的残差
l −1)
δ (n
=
i
=
=
∂
l −1)
∂z (n
i
J(W, b; x, y)
∂J(W, b; x, y)
l )
∂z (n
i
S l
∑
·
l )
∂z (n
i
l −1)
∂z (n
i
(2.13)
l −1)
δ (n j l ) W i (n j l −1) f ′ (z (n
)
i
j=1
将 n l − 1 与 n l 的关系替换为 l 与 l + 1 的关系,则可得到
δ (l)
i =

 S
l+1
 
 ∑
(l) (l+1) 
 f ′ (z (l) )

W
δ
J(W,
b;
x,
y)
=
ji j
i


(l)
∂
∂z i
(2.14)
j=1
若取函数 f 为 sigmoid 函数,则有
(l)
(l)
(l)
f ′ (z i (l) ) = f (z (l)
i ) ◦ (1 − f (z i )) = a i ◦ (1 − a i )
(2.15)
其中 ◦ 代表点乘。于是可得到 σ (l+1)
到 σ (l)
j
j 的递推式:
 S

l+1
 ∑

(l)
(l+1)

 (a (l) ◦ (1 − a (l) ))
W
δ
δ (l)
=
ji j
i
i

 i
(2.16)
j=1
反向传播,一般采用梯度下降法对每一层的权重进行调整,即
W i (l) j = W i (l) j − α
∂
∂W i (l) j
J(W, b; x, y)
第 11 页
(2.17)基于神经网络的花苗分类
其中,α 是学习率。因而需要求权重 W i (l) j 对于代价函数的偏导,此时可使用当前
层的残差来进行计算,即
(l+1)
∂
∂W i (l) j
J(W, b; x, y) =
∂J(W, b; x, y) z i
∂z i (l+1)
W i (l) j
(2.18)
又有
z i (l+1)
W i (l) j
(∑ S
)
(l)
(l)
j=1 W i j f (z i )
=
l
W i (l) j
(l)
= f (z (l)
i ) = a i
(2.19)
于是可得
∂
∂W i (l) j
(l+1)
J(W, b; x, y) = a (l)
j δ i
(2.20)
综上,可以总结 BP 神经网络算法 BP 神经网络算法
1 输入:训练输入 x,训练输出 y,学习率 α
2 while 未达到收敛条件
3
输入训练输入,训练输出,学习率
1. 初始化神经网络的权重与偏置
2. 对输入进行前向传播,得到除输入层外每一层(L 2 , · · · , L n l )的激活值
a (2) , · · · , a (n l )
3. 计算各层残差:
(1) 对输出层(第 n l 层)
δ (n l ) = −(y − a (n l ) ) · (a (l) ◦ (1 − a (l) ))
(2.21)
(2) 对于 l = n l − 1, · · · , 2 各层,可递推得出残差值
δ (l) = ((W (l) ) T δ (l+1) ) · (a (l) )
(2.22)
(3) 计算损失函数对每一层权重的偏导数值
∇ W (l) J(W, b; x, y) = δ (l+1) (a (l) ) T (2.23)
W (l) = W (l) − α∇ W (l) J(W, b; x, y) (2.24)
(4) 更新参数
4 end
若为多分类问题,先对 y 进行 one-hot 处理得到 p 维向量 (y 1 , y 2 , · · · , y p )(假
第 12 页基于神经网络的花苗分类
设 y 有 p 种取值),并将输出层的激活函数选为 softmax,即
(nl )
a i (n l )
f s (z i (n l ) )
=
e z i
= ∑ (nl )
z j
j e
(2.25)
并且代价函数使用交叉熵损失函数
J(W, b; x, y) = −
∑
l )
y i log a (n
i
(2.26)
i
则输出层残差为
l )
δ (n
=
i
=
∂J
∂z i (n l )
∑ ∂J
∂a i (n l )
·
a i (n l ) ∂z i (n l )
∑ ∂ − ∑ i y i log a (n l ) ∂a (n l )
i
i
· (n
=
(n l )
a i
∂z i l )
i
∑ y i ∂a (n l )
i
= −
(n l )
(n l )
∂z j
i a i
i
(nl )
∑
当 i = j 时,记 e z j = e A ,
∂a i (n l )
∂z (n j l )
(n )
z k l
k, j e
=
= e B ,显然有 e A + e B =
(2.27)
∑
(nl )
z i
i e
,于是
∂a (n j l )
∂z (n j l )
A
=
=
=
=
=
∂ e A e +e B
∂A
e (e B + e A ) − e 2A
(e A + e B ) 2
e A e B
(e A + e B ) 2
e A
e B
e A + e B e A + e B
e A
e A
(1
−
)
e A + e B
e A + e B
A
= a (n j l ) (1 − a (n j l ) )
第 13 页
(2.28)基于神经网络的花苗分类
2.3.3
激活函数
sigmoid sigmoid 函数表达式如下
f (x) =
1
1 + e −x
(2.29)
其图像如下图所示
sigmoid 激活函数考虑将输入值映射到 (0, 1) 的区间中,该函数在定义域内连续,
且导数大于 0。它也有较为简单的求导结果
f ′ (x) = f (x)(1 − f (x))
(2.30)
但是在神经网络中,特别是对于层数较多的网络,通常不采用 sigmoid 作为激活函
数,主要是因为其容易产生梯度消失的情况。当输入非常大或非常小的时候,其梯
度趋近于 0,反向传播的过程中直接导致梯度无法传播,无法有效地调整权重。虽
然做标准化可以让数据近似服从正态分布,但梯度消失仍有可能产生,在学习过
程中可能会产生输入较大或较小的情况。或许这个问题可以用 batch-normalization
来缓解,但明显采取一种更佳的激活函数是较为可取的做法。
ReLU ReLU 函数表达式如下
f (x) = max{0, x}
第 14 页
(2.31)基于神经网络的花苗分类
图像如下
其决定它有非常简单的求导结果





 1, x > 0
′
f (x) = 



 0, x < 0
(2.32)
RuLU 收敛能比 sigmoid 快的多,一方面其计算快,比起 sigmoid 函数的导数需要
指数运算,RuLU 只需要做大小的比较。另一方面,其梯度经过多个层传播之后,
多数能够保持原汁原味,比起 sigmoid 会梯度消失要好得多。然而,RuLU 也有弱
点,当 x < 0 时 f (x) 为 0,梯度为 0,这直接导致该神经元失活。因而在训练过程
中,要注意取较小的学习率。
Leaky ReLU Leaky ReLU 是针对 RuLU 的弱点而改进的,其考虑用一个比较小
的数去替代 x < 0 时的 f (x) = 0,即





 x, x > 0
′
f (x) = 



 ax, x < 0
图像如下
第 15 页
(2.33)基于神经网络的花苗分类
其求导结果为





 1, x > 0
′
f (x) = 



 a, x < 0
(2.34)
这个方法可以使 x < 0 处避免失活,但是额外引入了超参数 a。
PReLU PReLU 是针对 Leaky ReLU 的进一步优化,其考虑在反向传播过程中,
也对 a 进行学习,从而避免引入超参数 a。一些实验
[1]
证明这种优化能取到好的
学习效果。
2.3.4
传统 BP 网络的应用
以上介绍的 BP 网络的算法以及较为传统的结构,我们想探究随着图像尺寸
的变化(即输入大小)以及隐含层神经元。首先我们制备数据,通过 opencv 的方
法,将输入图像归一化为同一大小,分别为 64 × 64,96 × 96,128 × 128,学习率
设置 0.03,优化函数采用 Mini-batch,以 8 个样本作为一个 batch,epoch 设为 600。
首先考虑当隐含层分别设为 1000 和 500 时,图像大小为 64 × 64 时,模型的训练
准确率如下:
第 16 页基于神经网络的花苗分类
图像大小为 64 × 64 时,隐含层为 1000 和 500 的准确率图
从图中可以看出,隐含层为 1000 时比 500 好接近 5%,收敛速度上,前者在 epoch
为 300 时就趋于稳定,后者在 epoch 为 450 时趋于稳定。其原因四隐含层 1000 时,
其自由度比 500 大,随着参数的增加,更有可能得到偏差小的模型。从实验可以
看出,前者相比于后者在达到较低偏差的同时,其方差也不会很大。
当隐含层分别设为 1000 和 500 时,图像大小为 96 × 96 时,模型的训练准确
率如下:
图像大小为 96 × 96 时,隐含层为 1000 和 500 的准确率图
从图中可以看出,隐含层 1000 与 500 在准确率上持平,为 50% 左右。由于随着图
像的尺寸增加,过拟合的风险增大。而前者相比于后者有更低的模型复杂度,一
定程度上抵制了过拟合。而过拟合的风险随着图像尺寸的增大而增大的现象,我
们将在下图进一步看到:
当隐含层分别设为 1000 和 500 时,图像大小为 128 × 128 时,模型的训练准
确率如下:
第 17 页基于神经网络的花苗分类
图像大小为 128 × 128 时,隐含层为 1000 和 500 的准确率图
从图中可看出,当隐含层为 1000 时,其训练过程中准确率出现了大幅度的震荡,
而且准确率收敛在了 45% 左右,而隐含层为 500 的模型相比隐含层为 1000 的模
型的更加健壮,而且准确率接近 50%,比隐含层为 1000 的模型高了大概 4%。
综上,我们可以得到各个模型的准确率表格
64 × 64 96 × 96 128 × 128
1000 0.518188 0.522655 0.460115
500 0.460753 0.516273 0.48628
可以看出,在保证图像不要过大而导致过拟合下,隐含层 1000 的模型比隐含层
500 的模型性能更优。
2.3.5
梯度下降方法
梯度下降法的选取能影响收敛速度与质量,它也是模型构成的一部分。在应
用中一般有如下的梯度下降法可供选择
批量梯度下降法 批量梯度下降法(Batch Gradient Descent )考虑在计算了所有
样本之后再对参数进行更新,即
W
(l)
=
m
∑
W (l) − α∇ W (l) J(W, b; x (i) , y (i) )
(2.35)
i=1
由于通常训练的样本非常大,若在计算所有样本之后再进行参数更新,会让更新
的速度减慢。另外,模型实现一般会采用矩阵运算,BGD 占的内存会非常多,从
第 18 页基于神经网络的花苗分类
而影响计算速度。
随机梯度下降法 随机梯度下降法(Stochastic Gradient Descent )的想法与 BGD
截然不同,计算每一个样本之后便进行一次反向传播,对参数进行更新,即
W (l) = W (l) − α∇ W (l) J(W, b; x, y)
(2.36)
相比之下,SGD 的训练速度比 BGD 快得多,在 BGD 进行一次反向传播的时间
内,SGD 已经进行过多次传播。但是在梯度下降过程中,SGD 容易出现震荡,由
于单个样本并不能代表梯度最大的方向,也有可能导致解非最优的情况。
小批量梯度下降法 小批量梯度下降法(Mini-batch Gradient Descent )考虑了批
量梯度下降法和随机梯度下降法的优缺点,并进行结合,考虑将数据集划分成多
个含有较小数据的 batch,然后对这些 batch 分别采用 BGD。下面给出第 i 个 batch
的训练公式
W
(l)
=
m
∑
W (l) − α∇ W (l) J(W, b; x, y)
(2.37)
(x,y)∈b i
其中,b i 代表当前 batch 所包含的训练样本 (x, y) 的集合。
动量梯度下降法 无论是 SGD 还是 MGD,即便 MGD 已在 SGD 上做了优化,在
训练过程中仍可能会有振荡的风险。一种优化的方法是基于 SGD,在对参数 W (l)
进行更新时,会考虑上一次的更新幅度,若是当前的梯度方向与上一次的相同,
则能够加速收敛,反之则能抑制更新,这也是采用了动量的想法。其算法如下
1 输入:学习率 ε,动量参数 α
2 t dW = αt dW + (1 − α)t dW
3 W = W − εt dW
第 19 页基于神经网络的花苗分类
2.3.6
正则化与 dropout
机器学习中,常会发生过拟合的情况,通常引起这种情况的原因有数据量过
小、维度过大、模型复杂度过大等,而此现象是方差过大且偏差太小所致。通常
维度过大可采用特征选择的方法来降维,而模型复杂度可以用正则化项来限制。
它是考虑在损失函数中添加能反映出模型复杂度的项。例如在神经网络中,下面
的损失函数的第二项称为 L2 正则化
J(W, b; x, y) = −
∑
l )
y i log a (n
+ λ
i
i
∑
w 2
(2.38)
w
我们可以把损失函数看出是由偏差衡量项(第一项)和方差衡量项(第二项)组
成,其本质是偏差、方差权衡,权衡通过 λ 来实现。
除了 L2 正则化之外,常用的还有 L1 正则化,为如下形式
J(W, b; x, y) = −
∑
l )
+ λ
y i log a (n
i
i
∑
|w|
(2.39)
w
将正则化方法加入到神经网络中,设使用 96 × 96 的图像大小,隐含层神经
元个数为 500,学习率设置 0.03,优化函数采用 Mini-batch,以 8 个样本作为一个
batch,epoch 设为 600。我们依次测试当正则化系数为 0.1,0.01,0.001 和 0.0001 时
的模型差别,结果如下图所示
正则化系数为 0.1 和 0.01
第 20 页基于神经网络的花苗分类
正则化系数为 0.001 和 0.0001
其结果如下表
正则化系数 0.1 0.01 0.001 0.0001
准确率 0.0587109 0.102744 0.331844 0.49649
可以看出,只有一层隐含层的 BP 网络对于正则化系数很敏感。当取 0.1 和 0.01
时,模型太过简单,以至于得不到好的模型,当放宽到 0.0001 时,可以接近 50%。
相对而言,正则化用于复杂的模型效果会更好,例如卷积神经网络。
神经网络中,除了加入正则化项之外,还能考虑在每次训练中,让所有神经
元以一定概率失活,即封闭该神经元的输出,此方法成为 dropout。因而在每次训
练中,网络结构都不一样,在降低模型复杂度的同时,也是对于多个模型的集成,
其示意图如下。
dropout 工作原理示意图
第 21 页基于神经网络的花苗分类
2.3.7
BP 神经网络 +X
原始数据进行 SVM 的分类结果
svm,linear svm,poly svm,rbf svm,sigmoid
64 × 64 0.361199 0.524569 0.486280 0.081685
96 × 96 0.388641 0.534142 0.507339 0.086790
128 × 128 0.373325 0.559668 0.507977 0.356733
由于 SVM 在分类问题上取得很好的效果,考虑将 softmax 换成 SVM,有
softmax svm,poly nsvm,linear nsvm,poly nsvm,rbf nsvm,sigmoid
64 × 64,hid:1000 0.518188 0.524569 0.538609 0.576260 0.587109 0.081685
96 × 96,hid:1000 0.522655 0.534142 0.530951 0.577537 0.590938 0.075303
96 × 96,hid:500 0.516273 0.534142 0.507339 0.541799 0.580728 0.066369
96 × 96,hid:500,C=10 −4 0.49649 0.534142 0.529675 0.572431 0.594129 0.356733
为了更为直观展现结果,上表对应的条形图如下
第 22 页基于神经网络的花苗分类
BP 神经网络 +SVM。model A 指 64 × 64,隐含层神经元个数为 1000;model B 指
96 × 96,隐含层神经元个数为 1000;model C 指 96 × 96,隐含层神经元个数为
500;model D 指 96 × 96,隐含层神经元个数为 500,正则化系数为 0.0001。
尝试使用决策树代替 softmax,与 SVM 进行对比
model softmax nsvm,rbf nCART,mdh:5 nCART,mdh:10 nCART,mdh:15 nCART,mdh:20
A 0.518188 0.587109 0.414167 0.417358 0.408424 0.411615
B 0.522655 0.590938 0.416720 0.398851 0.387364 0.391078
C 0.516273 0.580728 0.353542 0.337588 0.325463 0.327378
D 0.49649 0.594129 0.395662 0.398851 0.389917 0.391193
为了更为直观展现结果,上表对应的条形图如下
第 23 页基于神经网络的花苗分类
BP 神经网络 +CART。model A 指 64 × 64,隐含层神经元个数为 1000;model B 指
96 × 96,隐含层神经元个数为 1000;model C 指 96 × 96,隐含层神经元个数为
500;model D 指 96 × 96,隐含层神经元个数为 500,正则化系数为 0.0001。
尝试使用随机森林代替 softmax,与 SVM 进行对比
model softmax nsvm,rbf nrf,20 nrf,50 nrf,80
A 0.518188 0.587109 0.544352 0.552010 0.573070
B 0.522655 0.590938 0.530313 0.559668 0.574346
C 0.516273 0.5580728 0.504786 0.534780 0.555839
D 0.49649 0.594129 0.523293 0.561583 0.555839
model nrf,110 nrf,140 nrf,170 nrf,200 nrf,230
A 0.574346 0.569879 0.577537 0.580089 0.574984
B 0.569879 0.586471 0.596043 0.583918 0.575622
C 0.545629 0.557754 0.550734 0.562221 0.569241
D 0.560944 0.573070 0.576899 0.580089 0.579451
第 24 页基于神经网络的花苗分类
BP 神经网络 +CART。model A 指 64 × 64,隐含层神经元个数为 1000;model B 指
96 × 96,隐含层神经元个数为 1000;model C 指 96 × 96,隐含层神经元个数为
500;model D 指 96 × 96,隐含层神经元个数为 500,正则化系数为 0.0001。
2.4
2.4.1
卷积神经网络
卷积神经网络概述
卷积神经网络的特点在于能够提取出一个图像中的各种特征。其原理为自然
图像的一部分的统计特性与其他部分是一样的。也就是说在这一部分学习的特征
也能用在另一部分上, 所以对于这个图像上的所有位置, 我们都能使用同样的学习
特征 (权值)。我们提取一种特征用一种卷积核, 卷积核为下图左边图像黄色部分,
卷积核的权值为黄色部分右下红色字体。
第 25 页基于神经网络的花苗分类
设大矩阵的大小为为 d × d, 利用大小为 m × m 的卷积核可以得到特征提取降维后
的大小为 (d − m + 1) × (d − m + 1) 的矩阵。这个过程为一个特征的提取。在卷积的
过程中, 从原图像 (Image) 矩阵 I 生成的卷积特征矩阵 C(Convolved Feature) 中的每
个元素为:
C i j =
m
m ∑
∑
w uv I i+u−1, j+v−1
(2.40)
u=1 v=1
其中,i, j ∈ (d − m + 1) 对于卷积特征矩阵 (Convolved feature) 我们下一步进行池
化。池化的目的是对图像不同位置进行聚合统计来描述大的图像。聚合统计可以
通过计算一个区域上某个特定特征的平均值或者最大值, 这样可以降低更多的维
度以及不容易过拟合。如果选择图像中连续的范围作为池化区域, 并且只是池化重
复的隐藏单元产生的特征, 那么这些池化单元具有平移不变性。这就意味着即使图
像经历了一个小的平移之后依然会产生相同的池化特征。池化过程如下图:
我们叫上图左图红色部分为一个池, 并且通常取能够将卷积特征矩阵平均划分的
大小的池。池化后我们得到池化特征矩阵 P(Pooled feature), 我们设卷积特征图像
第 26 页基于神经网络的花苗分类
为长宽都为 c 的矩阵, 则池长宽设为 p, 若为最大池化则 P 的元素为:
P i j =
max
{C u+(i−1)×(p+1),v+( j−1)×(p+1) }
u∈[1,p],v∈[1,p]
(2.41)
其中,i, j ∈ [1, cp ]。若采用平均池化,则 P 的元素为
p
p
1 ∑ ∑
P i j = 2
C u+(i−1)×(p+1),v+( j−1)×(p+1)
p v=1 u=1
(2.42)
其中,i, j ∈ [1, cp ]。事实上,在设置卷积核时,一般将其设置为四维,各个维度分
别为:卷积核长、卷积核宽、上一层的图像深度,卷积核个数。另外,对于一些
深度学习的任务,是需要重复卷积很多次,为了实现这一目的,需要确保卷积之
后图像长宽不变,于是在卷积之前通常在图像周围补足够个数的 0,以扩大图像
的尺寸,使得卷积之后的图像与原来的图像尺寸相同。
卷积神经网络其实可以包含两个大的部分,分别为特征提取层与分类器层。
特征提取层包含若干个卷积层和池化层。在特征提取层中,只需要训练卷积层,
而卷积层的共享参数与卷积核的属性,相比于全连接神经网络,参数更少,且抓
住了图像的特征。特征提取层的输出需要转化之后,才能接入分类器层,一般的
做法是将输出拉长为向量,而分类器层一般是用全连接的神经网络,最后接入
softmax 层,与标签计算损失函数,进而反向传播。下图是一种卷积网络结构,其
对应的任务是手写数字识别。
第 27 页基于神经网络的花苗分类
2.4.2
经典 CNN 模型
AlexNet 结构上由 8 层隐含层组成,前五层为特征提取层,后三层为分类器
层,用于做图像分类。其具体的结构如下:
AlexNet 结构,需注意的是,隐含层计算时分为上下部分计算,实际上是一个分布
式计算的想法。
AlexNet 当年提出来用已解决 ImageNet 的分类问题,对于 224 × 224 像素的三通
道照片,第一层使用 11 × 11 × 3 × 96 的卷积核;第二层使用 5 × 5 × 96 × 256 的
卷积核,并进行最大池化;第三层使用 3 × 3 × 256 × 384 的卷积核;第四层使用
3 × 3 × 384 × 384 的卷积核;第五层使用 3 × 3 × 384 × 256 的卷积核;之后连接全
连接层,第六、七层都为 4096 个神经元,第八层则使用 softmax。AlexNet 的创新
点在于激活函数采用了 ReLU 与 dropout。
从网络设计的思路上看,VGGNet 继承了 AlexNet 的思路,尝试建立层数更
多,深度更深的网络。其有一个很重要的特点是,VGGNet 的每个卷积层并不是
只做一次卷积操作,而是连续卷积 2 到 4 次。以下是两种 VGGNet 在 ImageNet 中
取得很好效果的结构,分别是 16 层版本和 19 层版本
第 28 页基于神经网络的花苗分类
VGG16 VGG19
2× conv3-64 2× conv3-64
max pooling max pooling
2× conv3-128 2× conv3-128
max pooling max pooling
3× conv3-256 4× conv3-256
max pooling max pooling
3× conv3-512 4× conv3-512
max pooling max pooling
3× conv3-512 4× conv3-512
max pooling max pooling
fc 4096 fc 4096
fc 4096 fc 4096
fc 1000 fc 1000
softmax softmax
其中,n×convX-Y 表示过滤器的边长为 X,深度为 Y,连续 n 层。
2.4.3
CNN 的应用
model
detail
accuracy
A B C D
2×conv3-64 1×conv3-64 1×conv3-64 1×conv3-64
max pooling max pooling max pooling max pooling
2×conv3-128 1×conv3-96 1×conv3-96 1×conv3-96
max pooling max pooling max pooling max pooling
2×conv3-256 2×conv3-128 1×conv3-128 1×conv3-128
max pooling max pooling max pooling max pooling
2×conv3-512 1×conv3-256 1×conv3-256 1×conv3-256
max pooling max pooling max pooling max pooling
2×conv3-512 1×conv3-512 2×conv3-512
max pooling max pooling max pooling
fc 1000 fc 1000 fc 1000 fc 500
softmax softmax softmax softmax
0.652202 0.664327 0.673899 0.659860
第 29 页基于神经网络的花苗分类
第 30 页基于神经网络的花苗分类
2.4.4
CNN+X
model softmax cnsvm,linear cnsvm,poly cnsvm,rbf cnsvm,sigmoid
E 0.652202 0.649649 0.654116 0.167837 0.110402
F 0.664327 0.642629 0.701340 0.219528 0.084876
G 0.673899 0.639438 0.693044 0.248883 0.156988
H 0.659860 0.624123 0.682195 0.375877 0.127632
model softmax cnsvm,poly nrf,20 nrf,50 nrf,80
E 0.652202 0.654116 0.644544 0.662412 0.671985
F 0.664327 0.701340 0.611997 0.659221 0.647096
G 0.673899 0.693044 0.622846 0.663689 0.672623
H 0.659860 0.682195 0.624761 0.652840 0.668156
model nrf,110 nrf,140 nrf,170 nrf,200 nrf,230
E 0.677090 0.694320 0.700702 0.693044 0.703255
F 0.657945 0.661136 0.664327 0.675176 0.665603
G 0.684110 0.673261 0.682195 0.677090 0.690491
H 0.669432 0.665603 0.677728 0.675176 0.686662
第 31 页基于神经网络的花苗分类
BP 神经网络 +CART。model A 指 64 × 64,隐含层神经元个数为 1000;model B 指
96 × 96,隐含层神经元个数为 1000;model C 指 96 × 96,隐含层神经元个数为
500;model D 指 96 × 96,隐含层神经元个数为 500,正则化系数为 0.0001。
2.5
参考文献
[1] Kaiming He,Delving Deep into Rectifiers: Surpassing Human-Level Perfor-
mance on ImageNet Classification,https://arxiv.org/abs/1502.01852
第 32 页基于神经网络的花苗分类
